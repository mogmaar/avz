<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Cloud | Avaaz Engineering blog]]></title>
  <link href="http://mogmaar.github.io/avz/blog/categories/cloud/atom.xml" rel="self"/>
  <link href="http://mogmaar.github.io/avz/"/>
  <updated>2014-05-23T16:41:31-07:00</updated>
  <id>http://mogmaar.github.io/avz/</id>
  <author>
    <name><![CDATA[Avaaz tech team]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Ravamping the Avaaz Email System]]></title>
    <link href="http://mogmaar.github.io/avz/blog/2014/05/23/email-revamp/"/>
    <updated>2014-05-23T16:25:15-07:00</updated>
    <id>http://mogmaar.github.io/avz/blog/2014/05/23/email-revamp</id>
    <content type="html"><![CDATA[<p>One of Avaaz’s biggest engineering challenges is to email our entire membership in a timely fashion. At 36 million members and counting, this is a formidable task, and over the past year we revamped the entire sending system from the ground up, greatly increasing sending speed and reliability. So let’s kick-off the Avaaz engineering blog and talk a bit about how that happened.</p>

<p>Avaaz’s Unique development environment
Avaaz has always used it’s own email sending software. We’ve had a few reasons &mdash; our core software has always been custom built and maintained by our own tech team since we launched in 2006. At the time, there was no system that could handle the level of multilingualization that we wanted in an integrated CMS/CRM while giving us the degree of flexibility of features we need, and that still seems to be the case. Our first email sending code was very simple, but it got the job done for 1 million, 2 million, 3 million users, and we improved and iterated it based on the needs of scale, as well as the needs of campaigns. (Does sending an email at 9am local time improve conversions? Let’s build it and see. Turns out it didn’t&hellip;)</p>

<p>The basic software used to email 3 million users was still being used to email 20 million users in the spring of 2013, and it was breaking a lot. The php blast engine code in the core app communicated with a Message Systems MTA that we ran out of our hosting company Datagram in NYC. A deep review of the system, including an audit by Message Systems, did not reveal significant improvements we could make to increase our sending rate beyond about 2 or 3 million emails per hour. Complicated targeting or certain donation settings could reduce that rate even more. That rate might seem fast, but Avaaz is built to ride global waves of public opinion which often only last a short amount of time, and we push the content of a campaign right up until the last minute. It’s not uncommon to ‘lock’ a campaign with less than 24 hours to go before a political decision point. It might take our language teams several hours to fully translate and test a campaign, meaning that we’d be crunched to send the non-English members (about ¾ of our list) a campaign with only hours to spare.</p>

<p>Rebuilding from the ground up
Once we committed to rebuild our entire system from the ground up, we decided on a strategy of renovating the house while continuing to live in it. We’d take functions that our core php code performed, write a stand-alone app with a service-oriented-architecture approach in our new cloud-based AWS system (we’ll save that upgrade for another blog post) and then turn on that feature. These new services would be independently scalable, perform discrete functions, have clear communication channels and operate as a ‘black box’ to the other apps around it. Email sending was our first priority and we built two apps to solve the problem.</p>

<p>The first simple app, blast (targeting) allocation, was a proof of concept for our new approach. The purpose of the app is to run a SQL query to target and ‘reserve’ a set of users for a blast version to be sent to at a later time. An allocated blast version could then sit on our site for a couple of hours or days until it is ready to be sent and campaigners could be confident that the makeup of the group of users targeted would not be affected by other blasting going on &mdash; very important for running accurate tests.</p>

<p>The task of allocating a blast version is a simple command &mdash; run a SELECT query and save the results. But at Avaaz scale, simple tasks get interesting. The query can be a large one that can take some time &mdash; it’s not unusual for the task to take 20-60 seconds or more depending on how complex the targeting is, and we sometimes have 20 or more campaigners working on setting up blasts at a time. So we setup RabbitMQ as the message queue to handle these requests. A campaigner clicks the ‘allocate’ button which sends a command to RabbitMQ. If there are requests ahead of it, the campaigner sees her allocation is ‘queued’. When a worker is free, it picks up the allocation request, processes it by running the blast allocation python code which uses the SQL to open up a socket connection to the database to stream the users to the blast allocation app, and saves the allocated users to the blast allocation database.</p>

<p>That looks like this: (MG to make simplified version of this image?)
[Image]</p>

<p>During this process we found this document, the 12 factor app, to be immensely helpful in aligning our development and sysadmin teams around the concept of our new system and we’d recommend it for anyone working on similar systems.</p>

<p>We’d successfully written an app that took a simple part of the core app and replaced it with an entirely different approach to software, proving the concepts of scalability, redundancy and flexibility that our re-write project was seeking. Now it was time for a more ambitious project &mdash; sending the emails.</p>

<p>Preparing and Injecting Emails
We’d already switched our MTA server away from the Message Systems and onto PowerMTA because we liked the degree of control and visibility it gave us. The task for the new ‘blast engine’ was to prepare the blast (take the blast template from the php app, fill in headers, content and user-specific tokens), and then inject it into the MTA with correctly formatted data and mark the emails as sent in the database.</p>

<p>We set out writing the python app to handle the above processes, as well as defining the connections to the campaigner interface, core php app, core database, and PowerMTA. It also needed to be able to handle different situations like tests to self, grouped blasts, failed blasts and transactional emails.</p>

<p>A key element of the system is breaking up large blasts into chunks. A typical blast version might be a million or more emails, but we process that by breaking it into a series of chunks.  When a campaigner clicks ‘send’ on a 1 million person email blast, a series of messages are sent to rabbit MQ, starting with prepare the content of the blast in chunks of 5,000. As workers process the chunks of 5,000 the prepared emails are stored in the blast engine’s own database until a worker is free to pick it up and inject it into the MTA.</p>

<p>Our initial set of configurations were guesses. We didn’t have answers to ‘what’s the ideal chunk size?’, ‘how many workers should we have?’, ‘how much capacity should the socket connection have?’ etc. After a couple months of development of the app and extensive testing, we were feeling ready. We had already demonstrated tests to fake users of speeds of almost 15 million per hour as well as sending to our entire Brazilian list of 5 million in 24 minutes. So we moved our code into production, feeling very proud about the massive speed increase we’d achieved for the campaigns team. Of course, our team immediately began to take advantage of the speed and push the limits, sending more emails in shorter succession. We quickly found a few optimizations to fix immediate bugs such as increasing the socket timeout limit, but it also unearthed a tricky bug that sent us back to the workshop for a few weeks.</p>

<p>We got hung up on the issues of synchronous execution and table locks. When a blast is sending, we lock the table with the users to be sent to so that workers on different chunks cannot pull the same user. Unfortunately, the amount of time it was taking to write was stacking up, causing the workers to wait. When we reached a certain, high, threshold of emails being sent, the table became stuck in ‘locked’ mode because all of the workers would be waiting for the table to unlock, causing fatal timeouts. The solution was use a unique ID field in the table of emails to be sent and then check that uniqueness as the blasts were going out. The problem of synchronous execution was solved by making more adjustments to what work actually needed to be done synchronously, and pushing some more functions into the asynchronous RabbitMQ tasks.</p>

<p>The new blast engine, on it’s second attempt at merging to production, has performed very well, with only occasional new bugs identified and then quickly fixed. With the 12-factor architecture, clear communication with other apps, and it’s own AWS instance and database, we are confident that the blast engine will be able to horizontally scale to hit our goal of emailing our entire membership in one hour &mdash; the blockers to this goal are currently on the email deliverability side, which we’re also very much working on addressing.
[Image}</p>

<p>So far the re-write of the Avaaz system has completed two apps, incorporating these key development components into our system and our team’s knowledge base.
The application code and infrastructure
Python, Flask (app framework), Celery (queue workers), Peewee (Database ORM)
Circus (process management), chaussette (WSGI server), custom socket server, 12factor environment
HTTP, Socket and Message Queue for communications
Encrypted channels (SSL) with per-request auth tokens (JWT)
The databases
Amazon RDS, multi-AZ
The message queues
RabbitMQ cluster, SSL</p>

<p>With the first few projects under our belt, we expect the pace of re-writing discreet peices of functionality as stand-alone, independently scalable apps to accelerate, gaining enough momentum to catch up to and add tremendous value to our current platform over the next two years, all while providing a more solid foundation for additional innovation projects to keep finding new ways to serve our community.</p>
]]></content>
  </entry>
  
</feed>
